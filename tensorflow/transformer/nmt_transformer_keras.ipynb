{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdrxqSLSvw_m"
      },
      "outputs": [],
      "source": [
        "# Install the nightly version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_text"
      ],
      "metadata": {
        "id": "8EnTOVEh-p6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset handling"
      ],
      "metadata": {
        "id": "Eq1tZSE3_aax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "2x_rh-nn_F_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "id": "hVJlkrDb_nOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  print('> Examples portuguese')\n",
        "  for pt in pt_examples.numpy():\n",
        "    print(pt.decode('utf-8'))\n",
        "  print()\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ],
      "metadata": {
        "id": "4MU_vsjh_tFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "XIzK3EpuGlF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "    f'{model_name}.zip',\n",
        "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")"
      ],
      "metadata": {
        "id": "sD1PJxLBGkrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers = tf.saved_model.load(model_name)"
      ],
      "metadata": {
        "id": "ngjhLQf2ANC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
      ],
      "metadata": {
        "id": "CBG8rZW6HEAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_examples"
      ],
      "metadata": {
        "id": "5fj5RnlNHMik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizers.en.tokenize(en_examples)"
      ],
      "metadata": {
        "id": "t3jwqA1WHZKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "id": "AYObFFv1Hkfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_examples"
      ],
      "metadata": {
        "id": "fABq9iAopYNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_pt = tokenizers.pt.tokenize(pt_examples)"
      ],
      "metadata": {
        "id": "fX6d6tK0pBq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_pt"
      ],
      "metadata": {
        "id": "yxpvjP93pM3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_pt = tokenizers.pt.detokenize(encoded_pt)"
      ],
      "metadata": {
        "id": "jkgSskhypQ8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_pt"
      ],
      "metadata": {
        "id": "qBIS5jp-pVip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_lengths = []\n",
        "en_lengths = []\n",
        "\n",
        "for pt_examples, en_examples in train_examples.batch(1024):\n",
        "  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n",
        "  pt_lengths.append(pt_tokens.row_lengths())\n",
        "\n",
        "  en_tokens = tokenizers.en.tokenize(en_examples)\n",
        "  en_lengths.append(en_tokens.row_lengths())"
      ],
      "metadata": {
        "id": "xLN1WtFAHlqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lengths = np.concatenate(pt_lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}')"
      ],
      "metadata": {
        "id": "uGoPZ04wIgHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lengths = np.concatenate(en_lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}')"
      ],
      "metadata": {
        "id": "3tq0pbl1qwP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup a data pipeline with tf.data"
      ],
      "metadata": {
        "id": "qtSd-eqER922"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 128\n",
        "def prepare_batch(pt, en):\n",
        "  pt = tokenizers.pt.tokenize(pt)\n",
        "  pt = pt[:, :MAX_TOKENS]\n",
        "  pt = pt.to_tensor() # Convert `RaggedTensor` to 0-padded dense Tensor\n",
        "\n",
        "  en = tokenizers.en.tokenize(en)\n",
        "  en = en[:, :(MAX_TOKENS+1)]\n",
        "  en_inputs = en[:, :-1].to_tensor() # Drop the [END]\n",
        "  en_labels = en[:, 1:].to_tensor() # Drop the [START]\n",
        "\n",
        "  return (pt, en_inputs), en_labels"
      ],
      "metadata": {
        "id": "6b2ZWvxBJMqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "YAFjXvfcTIDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  )"
      ],
      "metadata": {
        "id": "O3OoCM5XTxRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)"
      ],
      "metadata": {
        "id": "HoZ9XlR4UCFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (pt, en), en_labels in train_batches.take(1):\n",
        "  print(pt.shape)\n",
        "  print(en.shape)"
      ],
      "metadata": {
        "id": "u7LwbUIrUIGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "bkvX_1U-wdKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth / 2 # => 2/d_model\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis] # (hidden_dim, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)\n",
        "  angle_rads = positions * angle_rates\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1\n",
        "  )\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "YY6JR9QYUPG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_encoding = positional_encoding(length=2048, depth=512)\n",
        "\n",
        "# Check the shape.\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "# Plot the dimensions.\n",
        "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kNytopp8xjNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n",
        "p = pos_encoding[1000]\n",
        "dots = tf.einsum('pd,d -> p', pos_encoding, p)\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(dots)\n",
        "plt.ylim([0,1])\n",
        "plt.plot([950, 950, float('nan'), 1050, 1050],\n",
        "         [0,1,float('nan'),0,1], color='k', label='Zoom')\n",
        "plt.legend()\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(dots)\n",
        "plt.xlim([950, 1050])\n",
        "plt.ylim([0,1])"
      ],
      "metadata": {
        "id": "I6VLG3Qb4gwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embedding"
      ],
      "metadata": {
        "id": "PDWRtr84lb-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ],
      "metadata": {
        "id": "JQPuP6nslbdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_pt = PositionalEmbedding(tokenizers.pt.get_vocab_size(), d_model=512)\n",
        "embed_en = PositionalEmbedding(tokenizers.en.get_vocab_size(), d_model=512)"
      ],
      "metadata": {
        "id": "lvsuoqLgmY9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_emb = embed_pt(pt)\n",
        "en_emb = embed_en(en)"
      ],
      "metadata": {
        "id": "NcDU19rZnBKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_emb._keras_mask"
      ],
      "metadata": {
        "id": "Z5rehtIqnMxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the feed forward network"
      ],
      "metadata": {
        "id": "7IZsp3BPr--v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(\n",
        "    d_model, # Input/Output dimensionality\n",
        "    dff, # Inner-layer dimensionality\n",
        "):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'), # shape: (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model) # shape: (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "Rj8LgCPsnQte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ffn = point_wise_feed_forward_network(d_model=512, dff=2048)"
      ],
      "metadata": {
        "id": "gE-IFKNFsi_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_ffn(tf.random.uniform((64, 50, 512))).shape)"
      ],
      "metadata": {
        "id": "IB3_WAu3srHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exHwUcKpszfn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}