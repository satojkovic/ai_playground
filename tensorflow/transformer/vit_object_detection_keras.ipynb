{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vit_object_detection_keras",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-84PhEdo7vp"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import scipy.io\n",
        "import shutil"
      ],
      "metadata": {
        "id": "J-mXHuBxpFFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper parameters"
      ],
      "metadata": {
        "id": "oVqCjnjesn1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 224\n",
        "patch_size = 32\n",
        "\n",
        "input_shape = (image_size, image_size, 3)\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "# Size of the transformer layers\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 4\n",
        "mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers"
      ],
      "metadata": {
        "id": "SVEjE2g6sqFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dataset"
      ],
      "metadata": {
        "id": "soLtSqJ0oLCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_download_file = keras.utils.get_file(\n",
        "    fname='caltech_101_zipped',\n",
        "    origin=\"https://data.caltech.edu/tindfiles/serve/e41f5188-0b32-41fa-801b-d1e840915e80/\",\n",
        "    extract=True,\n",
        "    archive_format='zip',\n",
        "    cache_dir='./'\n",
        ")"
      ],
      "metadata": {
        "id": "wQfyJIMBpbip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.unpack_archive('datasets/caltech-101/101_ObjectCategories.tar.gz', './')"
      ],
      "metadata": {
        "id": "r3twGFcYqf_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.unpack_archive('datasets/caltech-101/Annotations.tar', './')"
      ],
      "metadata": {
        "id": "av1ChATfqiib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_images = '101_ObjectCategories/airplanes/'\n",
        "path_annot = 'Annotations/Airplanes_Side_2/'"
      ],
      "metadata": {
        "id": "d__-OJa6rWcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))]\n",
        "annot_paths = [f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))]"
      ],
      "metadata": {
        "id": "x1aCHDLxrkrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths.sort()\n",
        "annot_paths.sort()"
      ],
      "metadata": {
        "id": "vT1IORoXr9Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths[:10], annot_paths[:10]"
      ],
      "metadata": {
        "id": "LTxi54ABsSTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, targets = [], []\n",
        "for i in range(len(annot_paths)):\n",
        "  annot = scipy.io.loadmat(os.path.join(path_annot, annot_paths[i]))['box_coord'][0]\n",
        "  top_left_x, top_left_y = annot[2], annot[0]\n",
        "  bottom_right_x, bottom_right_y = annot[3], annot[1]\n",
        "\n",
        "  image = keras.utils.load_img(os.path.join(path_images, image_paths[i]))\n",
        "  (w, h) = image.size[:2]\n",
        "\n",
        "  # Resize train images\n",
        "  if i < int(len(annot_paths) * 0.8):\n",
        "    image = image.resize((image_size, image_size))\n",
        "\n",
        "  images.append(keras.utils.img_to_array(image))\n",
        "\n",
        "  # Apply relative scaling\n",
        "  targets.append((\n",
        "       float(top_left_x) / w,\n",
        "       float(top_left_y) / h,\n",
        "       float(bottom_right_x) / w,\n",
        "       float(bottom_right_y) / h\n",
        "  ))\n",
        "\n",
        "(x_train, y_train) = (\n",
        "  np.asarray(images[: int(len(images) * 0.8)]),\n",
        "  np.asarray(targets[: int(len(targets) * 0.8)])\n",
        ")\n",
        "(x_test, y_test) = (\n",
        "  np.asarray(images[int(len(images) * 0.8) :]),\n",
        "  np.asarray(targets[int(len(targets) * 0.8) :])\n",
        ")"
      ],
      "metadata": {
        "id": "-PJCRRV-sc8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP layer"
      ],
      "metadata": {
        "id": "R4LWAFURuoji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "  for units in hidden_units:\n",
        "    x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "dVVapS19un_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Patch creation layer"
      ],
      "metadata": {
        "id": "Qi3Zkiaju7qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "  def __init__(self, patch_size):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def call(self, images):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "        strides=[1, self.patch_size, self.patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='VALID'\n",
        "    )\n",
        "    return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])"
      ],
      "metadata": {
        "id": "tQW1ucquug4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Display patches"
      ],
      "metadata": {
        "id": "VWTa0QkbwqzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(x_train[0].astype('uint8'))\n",
        "plt.axis('off')\n",
        "\n",
        "patches = Patches(patch_size)(tf.convert_to_tensor([x_train[0]]))\n",
        "print(f'Image size: {image_size}x{image_size}')\n",
        "print(f'Patch_size: {patch_size}x{patch_size}')\n",
        "print(f'{patches.shape[1]} patches per image')\n",
        "print(f'{patches.shape[-1]} elements per patch')\n",
        "print(f'Pathces shape: {patches.shape}')\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "  ax = plt.subplot(n, n, i + 1)\n",
        "  patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "  plt.imshow(patch_img.numpy().astype('uint8'))\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "J2D95Rp-wlkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Patch encoder"
      ],
      "metadata": {
        "id": "EykWm1NcET9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "  def __init__(self, num_patches, projection_dim):\n",
        "    super().__init__()\n",
        "    self.num_patches = num_patches\n",
        "    self.projection = layers.Dense(projection_dim)\n",
        "    self.position_embedding = layers.Embedding(\n",
        "        input_dim=num_patches, output_dim=projection_dim\n",
        "    )\n",
        "\n",
        "  def call(self, patch):\n",
        "    positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "    encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "jc9VjP5-xsTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the ViT model"
      ],
      "metadata": {
        "id": "Ht0Ow4dZFXNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_object_detector(\n",
        "  input_shape,\n",
        "  patch_size,\n",
        "  num_patches,\n",
        "  projection_dim,\n",
        "  num_heads,\n",
        "  transformer_units,\n",
        "  transformer_layers,\n",
        "  mlp_head_units\n",
        "):\n",
        "  inputs = layers.Input(shape=input_shape)\n",
        "  patches = Patches(patch_size)(inputs)\n",
        "  encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "  for _ in range(transformer_layers):\n",
        "    # Layer norm\n",
        "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    # MHA\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads, projection_dim, dropout=0.1\n",
        "    )(x1, x1) # self attention\n",
        "    # Skip connection\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "    # Layer norm\n",
        "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "    # MLP\n",
        "    x3 = mlp(x3, transformer_units, 0.1)\n",
        "    # Skip connection\n",
        "    encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "  # Output of transformer blocks: [batch_size, num_patches, projection_dim]\n",
        "  # Create a [batch_size, projection_dim] tensor\n",
        "  #   step1: layer norm\n",
        "  #   step2: flatten [batch_size, num_patches * projection_dim]\n",
        "  representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "  representation = layers.Flatten()(representation)\n",
        "  representation = layers.Dropout(0.3)(representation)\n",
        "  print(representation.get_shape())\n",
        "\n",
        "  # mlp\n",
        "  features = mlp(representation, mlp_head_units, dropout_rate=0.3)\n",
        "  # Final four neurons that output bounding box\n",
        "  bounding_box = layers.Dense(4)(features)\n",
        "\n",
        "  return keras.Model(inputs=inputs, outputs=bounding_box)\n"
      ],
      "metadata": {
        "id": "u6Zby63cE7s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the experiment"
      ],
      "metadata": {
        "id": "s7JBpbvVJAdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n",
        "  optimizer = tfa.optimizers.AdamW(\n",
        "      learning_rate=learning_rate, weight_decay=weight_decay\n",
        "  )\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n",
        "\n",
        "  checkpoint_filepath = './'\n",
        "  checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "      checkpoint_filepath, monitor='val_loss',\n",
        "      save_best_only=True, save_weights_only=True\n",
        "  )\n",
        "\n",
        "  history = model.fit(\n",
        "      x=x_train, y=y_train,\n",
        "      batch_size=batch_size,\n",
        "      epochs=num_epochs,\n",
        "      validation_split=0.1,\n",
        "      callbacks=[\n",
        "        checkpoint_callback, keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  return history"
      ],
      "metadata": {
        "id": "eOTOUM-yI8-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_object_detector = create_vit_object_detector(\n",
        "    input_shape, patch_size, num_patches, projection_dim, num_heads,\n",
        "    transformer_units, transformer_layers, mlp_head_units\n",
        ")"
      ],
      "metadata": {
        "id": "AJ93ICw8KDcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = run_experiment(vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs)"
      ],
      "metadata": {
        "id": "HHr9cOl_K2Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0kPLXBDQLepv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}