{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_vivit",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUXjqTpw8ffo"
      },
      "outputs": [],
      "source": [
        "!pip install -qq medmnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import imageio\n",
        "import medmnist\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "keras.utils.set_random_seed(SEED)"
      ],
      "metadata": {
        "id": "soX1JP-WhNKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "DATASET_NAME = \"organmnist3d\"\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (28, 28, 28, 1)\n",
        "NUM_CLASSES = 11\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 60\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ],
      "metadata": {
        "id": "Lep_WOAQhRFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_prepare_dataset(data_info: dict):\n",
        "  data_path = keras.utils.get_file(origin=data_info['url'], md5_hash=data_info['MD5'])\n",
        "  with np.load(data_path) as data:\n",
        "    train_videos = data['train_images']\n",
        "    valid_videos = data['val_images']\n",
        "    test_videos = data['test_images']\n",
        "\n",
        "    train_labels = data['train_labels'].flatten()\n",
        "    valid_labels = data['val_labels'].flatten()\n",
        "    test_labels = data['test_labels'].flatten()\n",
        "\n",
        "  return (train_videos, train_labels), (valid_videos, valid_labels), (test_videos, test_labels)"
      ],
      "metadata": {
        "id": "5riyJOyk-j1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[DATASET_NAME]"
      ],
      "metadata": {
        "id": "vMV2MliJ_WYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dataset\n",
        "prepared_dataset = download_and_prepare_dataset(info)\n",
        "(train_videos, train_labels) = prepared_dataset[0]\n",
        "(valid_videos, valid_labels) = prepared_dataset[1]\n",
        "(test_videos, test_labels) = prepared_dataset[2]"
      ],
      "metadata": {
        "id": "hYhGDqRU_gAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train_videos.shape = {train_videos.shape}')\n",
        "print(f'train_labels.shape = {train_labels.shape}')\n",
        "\n",
        "print(f'valid_videos.shape = {valid_videos.shape}')\n",
        "print(f'valid_labels.shape = {valid_labels.shape}')\n",
        "\n",
        "print(f'test_videos.shape = {test_videos.shape}')\n",
        "print(f'test_labels.shape = {test_labels.shape}')"
      ],
      "metadata": {
        "id": "AW10QWnMxdcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['figure.figsize'] = [30, 15]"
      ],
      "metadata": {
        "id": "Dv5tlBMq_aNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(train_videos[1])):\n",
        "  plt.subplot(3, 10, i + 1)\n",
        "  plt.imshow(train_videos[0, i, :, :])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VPte3oVg_ffq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels[1])\n",
        "print(info['label'])"
      ],
      "metadata": {
        "id": "Y9rOe0P6BAtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tf.data pipeline"
      ],
      "metadata": {
        "id": "O-8dLOhc_3Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
        "  frames = tf.image.convert_image_dtype(\n",
        "      frames[\n",
        "             ..., tf.newaxis\n",
        "      ], tf.float32\n",
        "  )\n",
        "  label = tf.cast(label, tf.float32)\n",
        "  return frames, label"
      ],
      "metadata": {
        "id": "jiUbS1kK_xPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataloader(\n",
        "    videos: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = BATCH_SIZE\n",
        "):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "  if loader_type == 'train':\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "  dataloader = (\n",
        "      dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .batch(batch_size)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        "  )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "jdH1aE5uAYHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = prepare_dataloader(train_videos, train_labels, 'train')\n",
        "validloader = prepare_dataloader(valid_videos, valid_labels, 'valid')\n",
        "testloader = prepare_dataloader(test_videos, test_labels, 'test')"
      ],
      "metadata": {
        "id": "3XcC_l-_CrnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TubeletEmbedding(layers.Layer):\n",
        "  def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.projection = layers.Conv3D(\n",
        "        filters=embed_dim,\n",
        "        kernel_size=patch_size,\n",
        "        strides=patch_size,\n",
        "        padding='valid'\n",
        "    )\n",
        "    self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "  def call(self, videos):\n",
        "    projected_patches = self.projection(videos)\n",
        "    flattend_patches = self.flatten(projected_patches)\n",
        "    return flattend_patches"
      ],
      "metadata": {
        "id": "rrWpd-BHDPs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    _, num_tokens, _ = input_shape\n",
        "    self.position_embedding = layers.Embedding(\n",
        "        input_dim=num_tokens, output_dim=self.embed_dim\n",
        "    )\n",
        "    self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "  def call(self, encoded_tokens):\n",
        "    encoded_positions = self.position_embedding(self.positions)\n",
        "    encoded_tokens = encoded_tokens + encoded_positions\n",
        "    return encoded_tokens"
      ],
      "metadata": {
        "id": "bLdsg0u5ESD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video vision transformer with spatio-temporal attention"
      ],
      "metadata": {
        "id": "Arxh5izBF0Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    transformer_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=NUM_CLASSES\n",
        "):\n",
        "  inputs = layers.Input(shape=input_shape)\n",
        "  patches = tubelet_embedder(inputs)\n",
        "  encoded_patches = positional_encoder(patches)\n",
        "\n",
        "  # Create multiple layers of transformer block\n",
        "  for _ in range(transformer_layers):\n",
        "    # Layer norm and multi head self attention\n",
        "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim//num_heads,\n",
        "        dropout=0.1\n",
        "    )(x1, x1)\n",
        "\n",
        "    # Skip connection\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "    # Layer norm and MLP\n",
        "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "    x3 = keras.Sequential([\n",
        "                           layers.Dense(units=embed_dim*4, activation=tf.nn.gelu),\n",
        "                           layers.Dense(units=embed_dim, activation=tf.nn.gelu)\n",
        "    ])(x3)\n",
        "\n",
        "    # Skip connection\n",
        "    encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "  # Layer norm and global average pooling\n",
        "  representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "  representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "  # Classify outputs\n",
        "  outputs = layers.Dense(units=num_classes, activation='softmax')(representation)\n",
        "\n",
        "  # Create the keras model\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "onrZG2kjFgM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment():\n",
        "  model = create_vivit_classifier(\n",
        "      tubelet_embedder=TubeletEmbedding(\n",
        "          embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
        "      ),\n",
        "      positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM)\n",
        "  )\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=[\n",
        "               keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
        "               keras.metrics.SparseTopKCategoricalAccuracy(5, name='top-5-accuracy')\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  # Train the model\n",
        "  _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
        "  _, accuracy, top5_accuracy = model.evaluate(testloader)\n",
        "  print(f\"Test acc: {round(accuracy * 100, 2)}%\")\n",
        "  print(f\"Test top5 acc: {round(top5_accuracy * 100, 2)}%\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "EdD0tjX3ISqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = run_experiment()"
      ],
      "metadata": {
        "id": "O8I0vqVqJiCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "bLL_eQfvkPfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES_VIZ = 25\n",
        "testsamples, labels = next(iter(testloader))\n",
        "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]"
      ],
      "metadata": {
        "id": "A0tGHLkMKOIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truths = []\n",
        "preds = []\n",
        "videos = []\n",
        "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
        "  with io.BytesIO() as gif:\n",
        "    imageio.mimsave(gif, (testsample.numpy() * 255).astype('uint8'), 'GIF', fps=5)\n",
        "    videos.append(gif.getvalue())\n",
        "\n",
        "  output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
        "  pred = np.argmax(output, axis=0)\n",
        "\n",
        "  ground_truths.append(label.numpy().astype('int'))\n",
        "  preds.append(pred)"
      ],
      "metadata": {
        "id": "Ws4K1pn4kesI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_box_for_grid(image_widget, fit):\n",
        "  if fit is not None:\n",
        "    fit_str = '{}'.format(fit)\n",
        "  else:\n",
        "    fit_str = str(fit)\n",
        "\n",
        "  h = ipywidgets.HTML(value='' + str(fit_str) + '')\n",
        "\n",
        "  boxb = ipywidgets.widgets.Box()\n",
        "  boxb.children = [image_widget]\n",
        "\n",
        "  vb = ipywidgets.widgets.VBox()\n",
        "  vb.layout.align_items = 'center'\n",
        "  vb.children = [h, boxb]\n",
        "  return vb"
      ],
      "metadata": {
        "id": "WiZZTNxylRE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxes = []\n",
        "for i in range(NUM_SAMPLES_VIZ):\n",
        "  ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
        "  true_class = info['label'][str(ground_truths[i])]\n",
        "  pred_class = info['label'][str(preds[i])]\n",
        "  caption = f'T: {true_class} | P: {pred_class}'\n",
        "\n",
        "  boxes.append(make_box_for_grid(ib, caption))"
      ],
      "metadata": {
        "id": "2-Z1k_1Xl9WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ipywidgets.widgets.GridBox(\n",
        "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns='repeat(5, 200px)')\n",
        ")"
      ],
      "metadata": {
        "id": "dTgHHHVemgoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCF101 top5 dataset"
      ],
      "metadata": {
        "id": "9Pz0DsJjw3Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UCF_BATCH_SIZE = 64\n",
        "UCF_IMG_SIZE = 224"
      ],
      "metadata": {
        "id": "wv6TNJ5j0U7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2"
      ],
      "metadata": {
        "id": "s83Rcr_HxQXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
        "!tar xf ucf101_top5.tar.gz"
      ],
      "metadata": {
        "id": "7cBoe3yRmvH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print(f'Total videos for training: {len(train_df)}')\n",
        "print(f'Total videos for test: {len(test_df)}')\n",
        "\n",
        "train_df.sample(5)"
      ],
      "metadata": {
        "id": "mjArFeArxFlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_video_names = train_df.get('video_name')\n",
        "test_video_names = test_df.get('video_name')"
      ],
      "metadata": {
        "id": "vG5Pub8nxPCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_center_square(frame):\n",
        "  y, x = frame.shape[0:2]\n",
        "  min_dim = min(y, x)\n",
        "  start_x = (x // 2) - (min_dim // 2)\n",
        "  start_y = (y // 2) - (min_dim // 2)\n",
        "  return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
      ],
      "metadata": {
        "id": "E8XpQu4aH7ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(path, max_frames=0, resize=(UCF_IMG_SIZE, UCF_IMG_SIZE)):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames = []\n",
        "  try:\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame = crop_center_square(frame)\n",
        "      frame = cv2.resize(frame, resize)\n",
        "      frame = frame[:, :, [2, 1, 0]] # BGR2RGB\n",
        "      frames.append(frame)\n",
        "      if len(frames) == max_frames:\n",
        "        break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  return np.array(frames)"
      ],
      "metadata": {
        "id": "rvmTZCp7IegJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df['tag']) # not use UNK\n",
        ")\n",
        "print(label_processor.get_vocabulary())"
      ],
      "metadata": {
        "id": "BtaiOdcurNXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tags = label_processor(train_df['tag'])\n",
        "test_tags = label_processor(test_df['tag'])"
      ],
      "metadata": {
        "id": "Z8eftpt0tIRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tags.shape)\n",
        "print(test_tags.shape)"
      ],
      "metadata": {
        "id": "6HRAHlIHt6XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataloader_from_df(\n",
        "    videos,\n",
        "    labels,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = UCF_BATCH_SIZE\n",
        "):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "  if loader_type == 'train':\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "  dataloader = (\n",
        "      dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .batch(batch_size)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        "  )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "p5rQp_3FzAyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}