{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "investigate_vit",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gdown -q"
      ],
      "metadata": {
        "id": "oXAAI74lNFDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import zipfile\n",
        "import cv2\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "xdI0sv9zW8MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup configuration"
      ],
      "metadata": {
        "id": "NfQBNwSiXmFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RESOLUTION = 224\n",
        "PATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "5QSM3iONXlpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation & preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "yd2wJoyUWiBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LutucbozWd9T"
      },
      "outputs": [],
      "source": [
        "crop_layer = layers.CenterCrop(RESOLUTION, RESOLUTION)\n",
        "norm_layer = layers.Normalization(\n",
        "    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
        "    variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
        ")\n",
        "rescale_layer = layers.Rescaling(scale=1.0/127.5, offset=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image, model_type, size=RESOLUTION):\n",
        "  image = np.array(image)\n",
        "  image = tf.expand_dims(image, 0)\n",
        "\n",
        "  # If the `model_type` is ViT, rescale the image to [-1, 1]\n",
        "  if model_type == 'original_vit':\n",
        "    image = rescale_layer(image)\n",
        "\n",
        "  resize_size = int((256 / 224) * size)\n",
        "  image = tf.image.resize(image, (resize_size, resize_size), method='bicubic')\n",
        "\n",
        "  image = crop_layer(image)\n",
        "\n",
        "  # If the `model_type` is Deit or DINO, normalize the image\n",
        "  if model_type != 'original_vit':\n",
        "    image = norm_layer(image)\n",
        "\n",
        "  return image.numpy()"
      ],
      "metadata": {
        "id": "UzkyuGWdP-rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a test image and display it"
      ],
      "metadata": {
        "id": "yqQb01PURFry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_url(url, model_type):\n",
        "  response = requests.get(url)\n",
        "  image = Image.open(BytesIO(response.content))\n",
        "  preprocessed_image = preprocess_image(image, model_type)\n",
        "  return image, preprocessed_image"
      ],
      "metadata": {
        "id": "aJTId7UjRFL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_file = keras.utils.get_file(\n",
        "  origin=\"https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\"\n",
        ")"
      ],
      "metadata": {
        "id": "GnYkf9VGQ99S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(mapping_file, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "imagenet_int_to_str = [line.rstrip() for line in lines]"
      ],
      "metadata": {
        "id": "O36VRPKRR4DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://dl.fbaipublicfiles.com/dino/img.png\"\n",
        "image, preprocessed_image = load_image_from_url(img_url, model_type=\"original_vit\")\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_2O3XDVaSCr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a model\n",
        "\n",
        "This model was pretrained on the ImageNet-21k dataset and was then fine-tuned on the ImageNet-1k dataset"
      ],
      "metadata": {
        "id": "-6AKtP1_mI_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gdrive_model(model_id):\n",
        "  model_path = gdown.download(id=model_id, quiet=False)\n",
        "  with zipfile.ZipFile(model_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "  model_name = model_path.split('.')[0]\n",
        "  inputs = keras.Input((RESOLUTION, RESOLUTION, 3))\n",
        "  model = keras.models.load_model(model_name, compile=False)\n",
        "  outputs, attention_weights = model(inputs)\n",
        "  return keras.Model(inputs, outputs=[outputs, attention_weights])"
      ],
      "metadata": {
        "id": "7HjLxguimIpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_base_i21k_patch16_224 = get_gdrive_model(\"1mbtnliT3jRb3yJUHhbItWw8unfYZw8KJ\")\n",
        "print(\"Model loaded.\")"
      ],
      "metadata": {
        "id": "aVH3CCQLSFs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "jY7z2QwBn-o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, attention_score_dict = vit_base_i21k_patch16_224.predict(preprocessed_image)"
      ],
      "metadata": {
        "id": "MCesJRA0n-W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label = imagenet_int_to_str[int(np.argmax(predictions))]"
      ],
      "metadata": {
        "id": "U9MX2hvFnX5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_label)"
      ],
      "metadata": {
        "id": "ymEK9SKwoRys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention heatmaps"
      ],
      "metadata": {
        "id": "MPucVo-8uaPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_dino_base16 = get_gdrive_model(\"16_1oDm0PeCGJ_KGBG5UKVN7TsAtiRNrN\")"
      ],
      "metadata": {
        "id": "iqjSb60doTr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://dl.fbaipublicfiles.com/dino/img.png\"\n",
        "image, preprocessed_image = load_image_from_url(img_url, model_type='dino')"
      ],
      "metadata": {
        "id": "ilbb_NF-u6Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, attention_score_dict = vit_dino_base16.predict(preprocessed_image)"
      ],
      "metadata": {
        "id": "3jZWqTbtvJ0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_HEADS = 12"
      ],
      "metadata": {
        "id": "bFMVYN8fwx-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_heatmap(attention_score_dict, image, model_type='dino'):\n",
        "  num_tokens = 2 if 'distilled' in model_type else 1\n",
        "\n",
        "  # Sort the Transformer blocks in order of their depth.\n",
        "  attention_score_list = list(attention_score_dict.keys())\n",
        "  attention_score_list.sort(key=lambda x: int(x.split('_')[-2]), reverse=True)\n",
        "\n",
        "  # Process the attention maps for overlay\n",
        "  w_featmap = image.shape[2] // PATCH_SIZE\n",
        "  h_featmap = image.shape[1] // PATCH_SIZE\n",
        "  attention_scores = attention_score_dict[attention_score_list[0]]\n",
        "\n",
        "  # Taking the representation from [CLS] token\n",
        "  attentions = attention_scores[0, :, 0, num_tokens:].reshape(NUM_HEADS, -1)\n",
        "\n",
        "  # Reshape the attentions\n",
        "  attentions = attentions.reshape(NUM_HEADS, w_featmap, h_featmap)\n",
        "  attentions = attentions.transpose((1, 2, 0))\n",
        "\n",
        "  # Resize the attention patches to [224(14x16), 224]\n",
        "  attentions = tf.image.resize(\n",
        "      attentions, size=(h_featmap * PATCH_SIZE, w_featmap * PATCH_SIZE)\n",
        "  )\n",
        "\n",
        "  return attentions"
      ],
      "metadata": {
        "id": "Bsj4LyeCvRib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# De-normalize the image for visual clarity.\n",
        "in1k_mean = tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
        "in1k_std = tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
        "preprocessed_img_orig = (preprocessed_image * in1k_std) + in1k_mean\n",
        "preprocessed_img_orig = preprocessed_img_orig / 255.0\n",
        "preprocessed_img_orig = tf.clip_by_value(preprocessed_img_orig, 0.0, 1.0).numpy()\n",
        "\n",
        "# Generate the attention heatmaps.\n",
        "attentions = attention_heatmap(attention_score_dict, preprocessed_img_orig)\n",
        "\n",
        "# Plot the maps.\n",
        "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(13, 13))\n",
        "img_count = 0\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(4):\n",
        "        if img_count < len(attentions):\n",
        "            axes[i, j].imshow(preprocessed_img_orig[0])\n",
        "            axes[i, j].imshow(attentions[..., img_count], cmap=\"inferno\", alpha=0.6)\n",
        "            axes[i, j].title.set_text(f\"Attention head: {img_count}\")\n",
        "            axes[i, j].axis(\"off\")\n",
        "            img_count += 1"
      ],
      "metadata": {
        "id": "NI7bJO_2eP2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the learned filters"
      ],
      "metadata": {
        "id": "bOnBXrHPb0lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "projections = (\n",
        "    vit_base_i21k_patch16_224.layers[1]\n",
        "    .get_layer('projection')\n",
        "    .get_layer('conv_projection')\n",
        "    .kernel.numpy()\n",
        ")\n",
        "projection_dim = projections.shape[-1]\n",
        "patch_h, patch_w, patch_channels = projections.shape[:-1]\n",
        "\n",
        "scaled_projections = MinMaxScaler().fit_transform(\n",
        "    projections.reshape(-1, projection_dim)\n",
        ")\n",
        "\n",
        "# Reshape the scaled projections so that the leading\n",
        "# three dimensions resemble an image\n",
        "scaled_projections = scaled_projections.reshape(patch_h, patch_w, patch_channels, -1)\n",
        "\n",
        "# Visualize the first 128 filters of the learned projections\n",
        "fig, axes = plt.subplots(nrows=8, ncols=16, figsize=(13, 8))\n",
        "img_count = 0\n",
        "limit = 128\n",
        "\n",
        "for i in range(8):\n",
        "  for j in range(16):\n",
        "    if img_count < limit:\n",
        "      axes[i, j].imshow(scaled_projections[..., img_count])\n",
        "      axes[i, j].axis('off')\n",
        "      img_count += 1\n",
        "\n",
        "fig.tight_layout()\n"
      ],
      "metadata": {
        "id": "UQkfZidlb5O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N5NizvHfdrwL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}